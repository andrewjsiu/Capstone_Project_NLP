{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP: Using Word2Vec to Predict Review Usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_directory = os.path.join('C:/Users/andre/Documents/yelp_dataset_challenge_round9')\n",
    "intermediate_directory = os.path.join(data_directory, 'intermediate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(intermediate_directory, 'useful.csv'))\n",
    "luseful = np.log(df['useful'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,382 terms in the word2vec vocabulary.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')\n",
    "word2vec = Word2Vec.load(word2vec_filepath)\n",
    "\n",
    "print(u'{:,} terms in the word2vec vocabulary.'.format(len(word2vec.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a list of terms, index, and term counts from the word2vec model\n",
    "ordered_vocab = [(term, vocab.index, vocab.count) for term, vocab in word2vec.wv.vocab.items()]\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda ordered_vocab:ordered_vocab[2], reverse=True)\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creat a dictionary mapping each word to a 100-dimensional vector\n",
    "word_vectors = dict(list(zip(word2vec.wv.index2word, word2vec.wv.syn0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score, ShuffleSplit\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory, 'trigram_transformed_reviews_all.txt')\n",
    "\n",
    "X = []\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf-8') as f:\n",
    "    for review in f:\n",
    "        X.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer():\n",
    "    \n",
    "    \"\"\" Given a word to vector mapping, vectorize texts by taking the mean of all the word vectors for each document\"\"\"\n",
    "    \n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(list(word2vec.values())[0])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in review if w in self.word2vec] \n",
    "                   or [np.zeros(self.dim)], axis=0) \n",
    "                   for review in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfMeanVectorizer():\n",
    "    \n",
    "    \"\"\" Vectorize texts by taking the weighted average word vectors by their TF-IDF\"\"\"\n",
    "    \n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(list(word2vec.values())[0])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x:x)\n",
    "        tfidf.fit(X)\n",
    "        # Let an unseem word be as infrequent as the most infreqeunt word\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                                 for w in review if w in self.word2vec] or \n",
    "                                [np.zeros(self.dim)], axis=0) \n",
    "                         for review in X])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_rmse(model, X, y, cv=5, scoring='neg_mean_squared_error'):\n",
    "    \n",
    "    \"\"\" Compute an overall RMSE across all folds of cross validation\"\"\"\n",
    "    \n",
    "    return np.sqrt(np.mean(np.multiply(cross_val_score(model, X, y, cv=cv, scoring='neg_mean_squared_error'), -1)))\n",
    "    \n",
    "def RMSE(y_true, y_pred):\n",
    "    \n",
    "    \"\"\" Root Mean Squared Error\"\"\"\n",
    "    \n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def RMSLE(y_true, y_pred):\n",
    "    \n",
    "    \"\"\" Root Mean Squared Logarithmic Error\"\"\"\n",
    "    \n",
    "    return np.sqrt(np.mean(((np.log(y_true + 1) - np.log(y_pred + 1))**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression and shrinkage methods: Ridge and lasso\n",
    "\n",
    "lr_w2v = Pipeline([(\"w2v_vectorizer\", MeanEmbeddingVectorizer(word_vectors)), \n",
    "                   (\"lr\", LinearRegression())])\n",
    "lr_w2v_tfidf = Pipeline([(\"tfidf_w2v_vectorizer\", TfidfMeanVectorizer(word_vectors)),\n",
    "                        (\"lr\", LinearRegression())])\n",
    "ridge_w2v = Pipeline([(\"w2v_vectorizer\", MeanEmbeddingVectorizer(word_vectors)),\n",
    "                    (\"ridge\", Ridge(alpha=1))])\n",
    "ridge_w2v_tfidf = Pipeline([(\"tfidf_w2v_vectorizer\", TfidfMeanVectorizer(word_vectors)),\n",
    "                    (\"ridge\", Ridge(alpha=1))])                            \n",
    "rfr_w2v = Pipeline([(\"w2v_vectorizer\", MeanEmbeddingVectorizer(word_vectors)),\n",
    "                    (\"rfr\", RandomForestRegressor(n_estimators=100))])                            \n",
    "xgb_w2v = Pipeline([(\"w2v_vectorizer\", MeanEmbeddingVectorizer(word_vectors)),\n",
    "                    (\"xgb\", XGBRegressor(n_estimators=100))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model              RMSE_5cv\n",
      "---------------  ----------\n",
      "xgb_w2v              0.6037\n",
      "rfr_w2v              0.6130\n",
      "ridge_w2v_tfidf      0.6175\n",
      "ridge_w2v            0.6176\n",
      "lr_w2v_tfidf       175.3874\n",
      "lr_w2v             324.2977\n"
     ]
    }
   ],
   "source": [
    "w2v_models = [(\"lr_w2v\", lr_w2v), (\"lr_w2v_tfidf\", lr_w2v_tfidf),\n",
    "              (\"ridge_w2v\", ridge_w2v), (\"ridge_w2v_tfidf\", ridge_w2v_tfidf),\n",
    "              (\"rfr_w2v\", rfr_w2v), (\"xgb_w2v\", xgb_w2v)]\n",
    "\n",
    "w2v_rmse = sorted([(name, cv_rmse(model, X, luseful, cv=5)) \n",
    "                     for name, model in w2v_models], key=lambda x:x[1])\n",
    "\n",
    "print (tabulate(w2v_rmse, floatfmt=\".4f\", headers=(\"model\", \"RMSE_5cv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Most Useful or Useless Words?\n",
    "\n",
    "To find the usefulness of any single word, I first estimate a regression model and then use it to predict the number of useful votes each single word will obtain in the entire vocabulary. For both XGBoost and Ridge regressors, words that involve a dollar amount, 'price', 'co-pay' or 'cash' tend to be the most useful words, perhaps because they are actually informative and provide objective facts. Words like 'confidence', 'amazing' and 'excellent' are mere subjective feelings, so they are among the least useful of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(learning_rate=0.01, \n",
    "                    n_estimators=2000, \n",
    "                    max_depth=3,\n",
    "                    min_child_weight=3,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.6,\n",
    "                    reg_lambda=10,\n",
    "                    n_jobs=4,  \n",
    "                    random_state=0).fit(MeanEmbeddingVectorizer(word_vectors)\n",
    "                                        .fit(X, luseful).transform(X), luseful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the predicted usefulness of each word\n",
    "words = np.array(list(word_vectors.keys()))\n",
    "predicted = xgb.predict(list(word_vectors.values()))\n",
    "indices = np.argsort(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Useful Words      Predicted Useful Votes\n",
      "-------------------  ------------------------\n",
      "1                                    0.712784\n",
      "show_up                              0.688866\n",
      "even_though                          0.685041\n",
      "$_70                                 0.679914\n",
      "$_20                                 0.679213\n",
      "third                                0.676152\n",
      "$_30                                 0.667335\n",
      "$                                    0.662971\n",
      "20                                   0.63884\n",
      "decide                               0.634052\n"
     ]
    }
   ],
   "source": [
    "useful_words = words[indices[-10:]]\n",
    "useful_pred = predicted[indices[-10:]]\n",
    "df = pd.DataFrame({'Most Useful Words': useful_words, \n",
    "                   'Predicted Useful Votes': np.exp(useful_pred)-1})\n",
    "print (tabulate(df.sort_values('Predicted Useful Votes', ascending=False), \n",
    "                headers=df.columns, showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Useful Words      Predicted Useful Votes\n",
      "--------------------  ------------------------\n",
      "confidence                         -0.0401011\n",
      "sincerely                          -0.0324617\n",
      "constantly                         -0.0163424\n",
      "funky                              -0.0150891\n",
      "birth                              -0.0141224\n",
      "most_importantly                   -0.00772482\n",
      "also                               -0.00560182\n",
      "encouragement                       0.00176609\n",
      "kid                                 0.00631058\n",
      "glow                                0.00653279\n"
     ]
    }
   ],
   "source": [
    "useless_words = words[indices[:10]]\n",
    "useless_pred = predicted[indices[:10]]\n",
    "df = pd.DataFrame({'Least Useful Words': useless_words, \n",
    "                   'Predicted Useful Votes': np.exp(useless_pred)-1})\n",
    "print (tabulate(df, headers=df.columns, showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=1).fit(MeanEmbeddingVectorizer(word_vectors)\n",
    "                           .fit(X, luseful).transform(X), luseful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = np.array(list(word_vectors.keys()))\n",
    "predicted = ridge.predict(list(word_vectors.values()))\n",
    "indices = np.argsort(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Useful Words      Predicted Useful Votes\n",
      "-------------------  ------------------------\n",
      "parking                           1.19824e+07\n",
      "price                             5.673e+06\n",
      "few_minute                        2.51517e+06\n",
      "become                            1.29542e+06\n",
      "co_pay                            1.2212e+06\n",
      "than                         691790\n",
      "amount                       507229\n",
      "side                         247344\n",
      "cash                         179244\n",
      "pretty                       170521\n"
     ]
    }
   ],
   "source": [
    "useful_words = words[indices[-10:]]\n",
    "useful_pred = predicted[indices[-10:]]\n",
    "df = pd.DataFrame({'Most Useful Words': useful_words, \n",
    "                   'Predicted Useful Votes': np.exp(useful_pred)-1})\n",
    "print (tabulate(df.sort_values('Predicted Useful Votes', ascending=False), \n",
    "                headers=df.columns, showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Useful Words      Predicted Useful Votes\n",
      "--------------------  ------------------------\n",
      "exceptional                          -1\n",
      "excellent                            -1\n",
      "amazing                              -1\n",
      "anyone                               -1\n",
      "great                                -0.999999\n",
      "expert                               -0.999999\n",
      "outstanding                          -0.999999\n",
      "issue                                -0.999998\n",
      "incredible                           -0.999998\n",
      "wonderful                            -0.999998\n"
     ]
    }
   ],
   "source": [
    "useless_words = words[indices[:10]]\n",
    "useless_pred = predicted[indices[:10]]\n",
    "df = pd.DataFrame({'Least Useful Words': useless_words, \n",
    "                   'Predicted Useful Votes': np.exp(useless_pred)-1})\n",
    "print (tabulate(df, headers=df.columns, showindex=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
