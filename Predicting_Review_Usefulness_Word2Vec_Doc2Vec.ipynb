{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP: Analyzing Healthcare Reviews and Predicting Their Useful Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_directory = os.path.join('C:/Users/andre/Documents/', 'yelp_dataset_challenge_round9')\n",
    "businesses_filepath = os.path.join(data_directory, 'yelp_academic_dataset_business.json')\n",
    "review_json_filepath = os.path.join(data_directory, 'yelp_academic_dataset_review.json')\n",
    "intermediate_directory = os.path.join(data_directory, 'intermediate')\n",
    "review_txt_filepath = os.path.join(intermediate_directory, 'review_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,211 health & medical entities in the dataset.\n"
     ]
    }
   ],
   "source": [
    "healthcare_ids = []\n",
    "\n",
    "# open the businesses file\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    \n",
    "    # iterate through each line (json record) in the file\n",
    "    for business_json in f:\n",
    "        \n",
    "        # convert the json record to a Python dict\n",
    "        business = json.loads(business_json)\n",
    "        \n",
    "        # if this business has no categories or is not a target entity, skip to the next one\n",
    "        if business[u'categories'] is None or u'Health & Medical' not in business[u'categories']:\n",
    "            continue\n",
    "        # Remove businesses in BW, Germany\n",
    "        if u'BW' in business[u'state']:\n",
    "            continue\n",
    "        # Remove businesses that are restaurants, food and pets\n",
    "        if u'Restaurants' in business[u'categories'] or u'Food' in business[u'categories'] or 'Pets' in business[u'categories']:\n",
    "            continue\n",
    "            \n",
    "        # add the business id to our healthcare_ids set\n",
    "        healthcare_ids.append(business[u'business_id'])\n",
    "\n",
    "# Turn the list of ids into a set, which is faster for testing whether an element is in the set\n",
    "healthcare_ids = set(healthcare_ids)\n",
    "\n",
    "# print the number of unique ids in the dataset\n",
    "print ('{:,}'.format(len(healthcare_ids)), u'health & medical entities in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 114,556 healthcare reviews written to the new txt file.\n"
     ]
    }
   ],
   "source": [
    "# Create a new file that contains only the text from reviews about healthcare entities.\n",
    "# One review per line in the this new file.\n",
    "    \n",
    "review_count = 0\n",
    "useful = []\n",
    "stars = []\n",
    "\n",
    "# create & open a new file in write mode\n",
    "with codecs.open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "\n",
    "    # open the existing review json file\n",
    "    with codecs.open(review_json_filepath, encoding='utf_8') as review_json_file:\n",
    "\n",
    "        # loop through all reviews in the existing file and convert to dict\n",
    "        for review_json in review_json_file:\n",
    "            review = json.loads(review_json)\n",
    "\n",
    "            # if this review is not in the target set, skip to the next one\n",
    "            if review[u'business_id'] not in healthcare_ids:\n",
    "                continue\n",
    "\n",
    "            # write each review as a line in the new file\n",
    "            # escape newline characters in the original review text\n",
    "            if review[u'text'] is None:\n",
    "                print(review_count)\n",
    "            \n",
    "            review_txt_file.write(review[u'text'].replace('\\n', '\\\\n').replace('\\r','') + '\\n')\n",
    "            review_count += 1\n",
    "            useful.append(review[u'useful'])\n",
    "            stars.append(review[u'stars'])\n",
    "\n",
    "print (u'Text from {:,} healthcare reviews written to the new txt file.'.format(review_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Text\n",
    "\n",
    "Use spaCy to tokenize, lemmitize and remove stopwords from the text. Apply phrase modeling by looking for words that tend to appear one after another more frequently than by random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "unigram_sentences_filepath = os.path.join(intermediate_directory, 'unigram_sentences_all.txt')\n",
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')\n",
    "bigram_sentences_filepath = os.path.join(intermediate_directory, 'bigram_sentences_all.txt')\n",
    "trigram_model_filepath = os.path.join(intermediate_directory, 'trigram_model_all')\n",
    "trigram_sentences_filepath = os.path.join(intermediate_directory, 'trigram_sentences_all.txt')\n",
    "trigram_reviews_filepath = os.path.join(intermediate_directory, 'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"Eliminate tokens that are pure punctuation or white space\"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def person(token):\n",
    "    \"\"\"Remove tokens that are PERSON entities\"\"\"\n",
    "    \n",
    "    return token.ent_type_ == 'PERSON'\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"Generator function (iterator without storing all texts)\n",
    "    to read in reviews from file and return the original line breaks\"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "\n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"Generator function to use spaCy to parse reviews, lemmatize the text and yield sentences\"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename), batch_size=10000, n_threads=4):\n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent \n",
    "                             if not (punct_space(token) or person(token))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Segment reviews into sentences and normalize the text\n",
    "# Save the parsed sentences file on disk to avoid storing the entire corpus in RAM\n",
    "if 1 == 1:\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim's LineSentence class takes the format: one sentence = one line\n",
    "# words are preprocessed and separated by whitespace.\n",
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run a phrase model to link two-words phrases together\n",
    "if 1 == 1:\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "\n",
    "else:\n",
    "    bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply the bigram model to unigram sentences and create a text with bigram sentences\n",
    "if 1 == 1:\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf-8') as f:\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "else:\n",
    "    trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            f.write(trigram_sentence + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a transformed text into a new file, with one review per line\n",
    "if 1 == 1:\n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf-8') as f:\n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath), batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # Lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review \n",
    "                              if not (punct_space(token))]\n",
    "            \n",
    "            # Apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # Remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review \n",
    "                              if term not in spacy.en.language_data.STOP_WORDS]\n",
    "            \n",
    "            # Write the transformed review as a new line\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    # Train a word2vec model: 300-dimensional word vectors\n",
    "    word2vec = Word2Vec(size=300, window=5, min_count=20, sg=0, workers=4)\n",
    "    word2vec.build_vocab(trigram_sentences) \n",
    "    for epoch in range(10):\n",
    "        word2vec.train(trigram_sentences)\n",
    "    word2vec.save(word2vec_filepath)\n",
    "\n",
    "word2vec = Word2Vec.load(word2vec_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,242 terms in the word2vec vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print(u'{:,} terms in the word2vec vocabulary.'.format(len(word2vec.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a list of terms, index, and term counts from the word2vec model\n",
    "ordered_vocab = [(term, vocab.index, vocab.count) for term, vocab in word2vec.wv.vocab.items()]\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda ordered_vocab:ordered_vocab[2], reverse=True)\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creat a dictionary mapping each word to a 300-dimensional vector\n",
    "word_vectors = dict(list(zip(word2vec.wv.index2word, word2vec.wv.syn0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pediatric_dentist', 0.6470401287078857),\n",
       " ('orthodontist', 0.6364657878875732),\n",
       " ('chiropractor', 0.6113377809524536),\n",
       " ('dermatologist', 0.6081831455230713),\n",
       " ('doctor', 0.6010243892669678)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['dentist'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pediatrician', 0.49545931816101074),\n",
       " ('obgyn', 0.4893958270549774),\n",
       " ('doctor', 0.48449233174324036),\n",
       " ('ob', 0.4684830904006958),\n",
       " ('gyno', 0.46404534578323364)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['dentist', 'baby'], negative=['tooth'],topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modeling with Document Features Created from Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score, ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful = np.array(useful)\n",
    "luseful = np.log(useful+1)\n",
    "X = []\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf-8') as f:\n",
    "    for review in f:\n",
    "        X.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer():\n",
    "    \"\"\" Given a word to vector mapping, vectorize texts \n",
    "    by taking the mean of all the word vectors for each document\"\"\"\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(list(word2vec.values())[0])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in review if w in self.word2vec] \n",
    "                   or [np.zeros(self.dim)], axis=0) \n",
    "                   for review in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfMeanVectorizer():\n",
    "    \"\"\" Weight average word vector features by its tf-idf\"\"\"\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(list(word2vec.values())[0])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x:x)\n",
    "        tfidf.fit(X)\n",
    "        # Let an unseem word be as infrequent as the most infreqeunt word\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                                 for w in review if w in self.word2vec] or \n",
    "                                [np.zeros(self.dim)], axis=0) \n",
    "                         for review in X])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cv_score(model, X, y, cv, scoring):\n",
    "    scores = []\n",
    "    for train, test in KFold(cv).split(X):\n",
    "        model.fit(X[train], y[train])\n",
    "        scores.append(scoring(y[test], model.predict(X[test])))\n",
    "    return np.array(scores)\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    \"\"\" Root Mean Squared Error\"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def RMSLE(y_true, y_pred):\n",
    "    \"\"\" Root Mean Squared Logarithmic Error\"\"\"\n",
    "    return np.sqrt(np.mean(((np.log(y_true + 1) - np.log(y_pred + 1))**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_w2v = Pipeline([(\"w2v_vectorizer\", MeanEmbeddingVectorizer(word_vectors)), \n",
    "                   (\"lr\", LinearRegression())])\n",
    "lr_w2v_tfidf = Pipeline([(\"tfidf_w2v_vectorizer\", TfidfMeanVectorizer(word_vectors)),\n",
    "                        (\"lr\", LinearRegression())])\n",
    "gbr_w2v = Pipeline([(\"w2v_vectorizer\", MeanEmbeddingVectorizer(word_vectors)),\n",
    "                    (\"gbr\", GradientBoostingRegressor(n_estimators=200))])\n",
    "gbr_w2v_tfidf = Pipeline([(\"tfidf_w2v_vectorizer\", TfidfMeanVectorizer(word_vectors)),\n",
    "                    (\"gbr\", GradientBoostingRegressor(n_estimators=200))])\n",
    "etr_w2v = Pipeline([(\"w2v_vectorizer\", MeanEmbeddingVectorizer(word_vectors)),\n",
    "                    (\"etr\", ExtraTreesRegressor(n_estimators=200))])\n",
    "etr_w2v_tfidf = Pipeline([(\"tfidf_w2v_vectorizer\", TfidfMeanVectorizer(word_vectors)),\n",
    "                    (\"etr\", ExtraTreesRegressor(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model             score\n",
      "-------------  --------\n",
      "gbr_w2v_tfidf    0.5995\n",
      "gbr_w2v          0.6003\n",
      "lr_w2v         164.0630\n",
      "etr_w2v        164.0630\n",
      "lr_w2v_tfidf   337.8631\n",
      "etr_w2v_tfidf  337.8631\n"
     ]
    }
   ],
   "source": [
    "w2v_models = [(\"lr_w2v\", lr_w2v), (\"lr_w2v_tfidf\", lr_w2v_tfidf),\n",
    "              (\"gbr_w2v\", gbr_w2v), (\"gbr_w2v_tfidf\", gbr_w2v_tfidf),\n",
    "              (\"etr_w2v\", lr_w2v), (\"etr_w2v_tfidf\", lr_w2v_tfidf)]\n",
    "\n",
    "w2v_scores = sorted([(name, np.sqrt(np.multiply(cross_val_score(\n",
    "    model, np.array(X), luseful, cv=3, scoring='neg_mean_squared_error'), -1)).mean()) \n",
    "                     for name, model in w2v_models], key=lambda x:x[1])\n",
    "\n",
    "print (tabulate(w2v_scores, floatfmt=\".4f\", headers=(\"model\", \"score\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modeling with Doc2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim's Doc2Vec class creates vector representations for entire documents\n",
    "# The input object is an iterator of LineSentence objects\n",
    "# The default dm=1 refers to the distributed memory algorithm\n",
    "# The algorithm runs through sentences twice: (1) build the vocab, \n",
    "# (2) learn a vector representation for each word and for each label (sentence)\n",
    "# Better results can be achieved by iterating over the data several times\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "doc2vec_filepath = os.path.join(intermediate_directory, 'doc2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['Review_' + str(x) for x in range(len(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DocIterator():\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield LabeledSentence(words=doc.split(), tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_reviews = DocIterator(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To produce better results, iterate over the data 10 times\n",
    "# and control the learning rate for each iteration\n",
    "if 0 == 1:\n",
    "    doc2vec = Doc2Vec(size=100, alpha=0.025, min_alpha=0.025, window=8, min_count=5, workers=4, iter=10)\n",
    "    doc2vec.build_vocab(labeled_reviews)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        doc2vec.train(labeled_reviews)\n",
    "        doc2vec.alpha -= 0.002\n",
    "        doc2vec.min_alpha = doc2vec.alpha\n",
    "    \n",
    "    doc2vec.save(doc2vec_filepath)\n",
    "    doc2vec.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "doc2vec = Doc2Vec.load(doc2vec_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ob', 0.7404353618621826),\n",
       " ('pcp', 0.7138863801956177),\n",
       " ('obgyn', 0.7098487019538879),\n",
       " ('ob/gyn', 0.7045339941978455),\n",
       " ('primary_care_physician', 0.6940240859985352)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.most_similar(positive=[\"pediatrician\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dentistry', 0.54587322473526),\n",
       " ('pediatric_dentist', 0.5437402129173279),\n",
       " ('dentist', 0.5426872372627258),\n",
       " ('wisdom_tooth', 0.5292142629623413),\n",
       " ('teeth', 0.5250543355941772)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.most_similar(positive=['pediatrician','tooth'], negative=['baby'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Review_70758', 0.4582338333129883),\n",
       " ('Review_108741', 0.4582274556159973),\n",
       " ('Review_40339', 0.45029744505882263),\n",
       " ('Review_100556', 0.43604543805122375),\n",
       " ('Review_68671', 0.43078291416168213)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.docvecs.most_similar('Review_10', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_d2v = Pipeline([(\"lr\", LinearRegression())])\n",
    "gbr_d2v = Pipeline([(\"gbr\", GradientBoostingRegressor(n_estimators=200))])\n",
    "etr_d2v = Pipeline([(\"etr\", ExtraTreesRegressor(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d2v_models = [(\"lr_d2v\", lr_d2v), (\"gbr_d2v\", gbr_d2v), (\"etr_d2v\", etr_d2v)]\n",
    "\n",
    "d2v_scores = [(name, np.sqrt(np.multiply(cross_val_score(\n",
    "    model, np.array(doc2vec.docvecs), luseful, cv=3, scoring='neg_mean_squared_error'), -1)).mean()) \n",
    "              for name, model in d2v_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = sorted(w2v_scores + d2v_scores, key=lambda x:x[1])\n",
    "\n",
    "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", \"score\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Review Usefulness Based on the Amount of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(model, X, y, n):\n",
    "    ss = ShuffleSplit(n_splits=3, test_size = 1 - n, random_state=0)\n",
    "    scores = []\n",
    "    for train, test in ss.split(X,y):\n",
    "        scores.append(RMSE(y[test], model.fit(X[train],y[train]).predict(X[test])))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sizes = [0.3, 0.5, 0.7]\n",
    "table = []\n",
    "for name, model in w2v_models:\n",
    "    for n in train_sizes:\n",
    "        table.append({'model': name, \n",
    "                      'RMSE': benchmark(model, np.array(X), luseful, n),\n",
    "                      'train_size': n})\n",
    "for name, model in d2v_models:\n",
    "    for n in train_sizes:\n",
    "        table.append({'model': name, \n",
    "                      'RMSE': benchmark(model, np.array(doc2vec.docvecs), luseful, n),\n",
    "                      'train_size': n})\n",
    "df = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,5))\n",
    "fig = sns.pointplot(x='train_size', y='RMSE', hue='model',\n",
    "                   data=df[df.model.map(\n",
    "                       lambda x: x in ['gbr_w2v_tfidf','gbr_d2v', 'etr_w2v_tfidf', 'etr_d2v'])])\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "fig.set(ylabel='RMSE')\n",
    "fig.set(xlabel='Training Size')\n",
    "fig.set(title='Model Comparison By Training Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
